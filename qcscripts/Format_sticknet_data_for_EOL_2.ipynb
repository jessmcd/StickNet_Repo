{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting StickNet data - for EOL preparation\n",
    "\n",
    "Originally written by Aaron Hill. Updated/optimized by Jessica McDonald April 2020\n",
    "\n",
    "This output of this script is what we upload to EOL. \n",
    "\n",
    "This script reads in the QC'd StickNet data (from QC_sticknet_data_for_EOL_1.ipynb) and divides it into daily files. These files are named \"0XXXA_YYYYMMDD.csv\". The contain that day's data, plus the first data point from the next day for StickNet 0XXXA. \n",
    "\n",
    "The user must edit the data directory, output directory, and biases. If you don't want to make any bias corrections, comment out that section in the main code. \n",
    "\n",
    "- - - - -\n",
    "If you don't want to use the EOL 10 Hz data for your own research, use the second half of this script to average the data\n",
    "to larger sampling intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# USER EDIT HERE =>\n",
    "\n",
    "data_directory = r'/Users/jessmcd/Documents/other/20170430_RapidProbeData/20170501_masstest/masstest/reformat/'\n",
    "output_directory= r'/Users/jessmcd/Documents/other/20170430_RapidProbeData/20170501_masstest/'\n",
    "\n",
    "#####\n",
    "# IF YOU HAVE BIASES, PLEASE FILL IN THE FOLLOWING FUNCTION.\n",
    "# UPDATE FUNCTION NAME IN MAIN CODE\n",
    "\n",
    "\n",
    "def SN_biases_Meso1819(steso):\n",
    "    #                    T .   RH .      P .   SP .  Dir\n",
    "    biases = {'0101A': [0.0,   0.0,    -0.5,    0.0,  0.0],\n",
    "              '0102A': [0.0,   -2.0,   0.0,    0.0,  0.0],\n",
    "              '0103A': [0.0,   0.75,   0.0,    0.0,  0.0],\n",
    "              '0104A': [0.0,   0.0,    -1.25,  0.0,  0.0],\n",
    "              '0105A': [-0.5,  0.5,    -1.0,   0.0,  0.0],\n",
    "              '0106A': [0.0,   2.5,    0.0,    0.0,  0.0],\n",
    "              '0107A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0108A': [0.0,   1.0,    0.0,    0.0,  0.0],\n",
    "              '0109A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0110A': [0.0,   0.0,    0.5,    0.0,  0.0],\n",
    "              '0111A': [0.0,   -1.25,  0.75,   0.0,  0.0],\n",
    "              '0112A': [0.0,   -0.75,  -0.5,   0.0,  0.0],\n",
    "              '0213A': [0.0,   -3.0,   -1.75,  0.0,  0.0],\n",
    "              '0214A': [0.0,   -0.5,   0.5,    0.0,  0.0],\n",
    "              '0215A': [0.0,   0.5,    -0.5,   0.0,  0.0],\n",
    "              '0216A': [0.0,   0.5,    0.5,    0.0,  0.0],\n",
    "              '0217A': [-0.5,  -1.0,   0.0,    0.0,  0.0],\n",
    "              '0218A': [0.0,   0.0,    0.5,    0.0,  0.0],\n",
    "              '0219A': [-0.5,  3.0,    0.5,    0.0,  0.0],\n",
    "              '0220A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0221A': [0.5,   0.0,    1.75,   0.0,  0.0],\n",
    "              '0222A': [0.0,   4.75,   1.5,    0.0,  0.0],\n",
    "              '0223A': [0.0,   -0.75,  -1.0,   0.0,  0.0],\n",
    "              '0224A': [0.0,   0.0,    0.0,    0.0,  0.0]\n",
    "             }\n",
    "\n",
    "    tbias  = biases[steso[0:5]][0]\n",
    "    rhbias = biases[steso[0:5]][1]\n",
    "    pbias  = biases[steso[0:5]][2]\n",
    "\n",
    "    print('\\nBIASES: Temperature {}, RH {}, Pressure {}'.format(tbias,rhbias,pbias))\n",
    "    \n",
    "    return tbias,rhbias,pbias\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#      FUNCTIONS\n",
    "#######################################################\n",
    "\n",
    "def parse_times(time_str, year, month):\n",
    "    '''\n",
    "    time_str:    sn['time']\n",
    "    year:        YYYY\n",
    "    month:       MM\n",
    "    \n",
    "    adds the str YYYYMM to the start of each time string (dd_HHMMSS.f). Changes MM if month \n",
    "    change occurs (and updates YYYY if month change is from 12 to 1)\n",
    "    \n",
    "    This takes about 20 seconds for a 300mb file. Why use this function instead of passing \n",
    "    pd.read_csv a parsing function? Because read_csv takes about 50 seconds for the same sized file.\n",
    "    \n",
    "    I SPENT HOURS ON THIS FUNCTION PLEASE USE IT. I HATE TIME STRINGS. '''\n",
    "    \n",
    "    try:\n",
    "        # find where you have start of new month\n",
    "        idx = np.where(time_str == '01_000000.0')[0][0]\n",
    "\n",
    "        # all data before new month, append YYYYMM to start of all time strings \n",
    "        date1 = [f'{year}{month}']*idx + time_str[:idx]\n",
    "        month = month +1\n",
    "        \n",
    "        if month==13:\n",
    "            month=1\n",
    "            year+=1\n",
    "          \n",
    "        print(f'\\n     New month found: {month}/{year}')\n",
    "        \n",
    "        # all data after new month, append YYYYMM+1 to start of all time strings \n",
    "        date2 = [f'{year}{str(month).zfill(2)}']*(len(time_str)-idx) + time_str[idx:]\n",
    "        \n",
    "        # put full dataset back together\n",
    "        full_date = date1.append(date2)\n",
    "        \n",
    "    except IndexError: # (there is no new month)\n",
    "        # append YYYYMM to start of all time strings \n",
    "        full_date = [f'{year}{month}']*len(time_str)+ time_str\n",
    "\n",
    "    return pd.to_datetime(full_date.values, format='%Y%m%d_%H%M%S.%f')   \n",
    "\n",
    "\n",
    "def read_sn(filename):\n",
    "    ''' \n",
    "    filename:  path to sticknet file (.txt)\n",
    "    \n",
    "    This function reads in the QCd sticknet data. It uses the parse_times function to \n",
    "    reformat the times, and sets the times as the index. This takes ~10 seconds per 100 MB. \n",
    "    A pandas DataFrame containing the sticknet data is returned. '''\n",
    "    \n",
    "    year = int(filename[-19:-15])\n",
    "    month = int(filename[-15:-13])\n",
    "    day = int(filename[-13:-11])\n",
    "    hour = int(filename[-10:-8])\n",
    "    minute = int(filename[-8:-6])\n",
    "    probe = filename[-25:-21]\n",
    "    \n",
    "    \n",
    "    sn = pd.read_csv(filename, names=['time','T','RH','P','windsp','winddir', 'batt', 'TFLAG', 'WFLAG'],\n",
    "                        header=1, error_bad_lines=False)\n",
    "    \n",
    "    sn['ID'] = pd.Series(probe,index=sn.index)\n",
    "\n",
    "    #dates = parse_times(sn['time'], year, month)\n",
    "    #sn.index = dates\n",
    "    \n",
    "    # for new sticknets that have year and month info, thank goodness\n",
    "    sn.index = pd.to_datetime(sn['time'], format='%Y%m%d_%H%M%S.%f')#20170430_181914.4\n",
    "    \n",
    "    sn.drop('time',axis=1,inplace=True)\n",
    "    \n",
    "    return sn\n",
    "\n",
    "def get_sticknet_ID(num):\n",
    "    ''' formats a proper sticknet ID based on an integer'''\n",
    "    \n",
    "    if num > 12:\n",
    "        probe_id = \"02{0}A\".format(\"%02d\"%num)\n",
    "    else:\n",
    "        probe_id = \"01{0}A\".format(\"%02d\"%num)\n",
    "        \n",
    "    return probe_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "0218A\n",
      "reading file 0218A_20170501_150634.txt ...\n",
      "     Data from 01 May 2017 1506 UTC to 01 May 2017 2138 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n",
      "-----\n",
      "0219A\n",
      "reading file 0219A_20170501_150839.txt ...\n",
      "     Data from 01 May 2017 1508 UTC to 01 May 2017 2336 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n",
      "-----\n",
      "0220A\n",
      "reading file 0220A_20170501_150625.txt ...\n",
      "     Data from 01 May 2017 1506 UTC to 01 May 2017 2336 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n",
      "-----\n",
      "0221A\n",
      "reading file 0221A_20170501_150634.txt ...\n",
      "     Data from 01 May 2017 1506 UTC to 01 May 2017 2336 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n",
      "-----\n",
      "0222A\n",
      "reading file 0222A_20170501_150645.txt ...\n",
      "     Data from 01 May 2017 1506 UTC to 01 May 2017 2048 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n",
      "-----\n",
      "0224A\n",
      "reading file 0224A_20170501_150633.txt ...\n",
      "     Data from 01 May 2017 1506 UTC to 01 May 2017 2330 UTC\n",
      "\n",
      " start of all data: 2017-05-01\n",
      " end of all data:   2017-05-02 \n",
      "\n",
      "Writing .csv file for 20170501\n"
     ]
    }
   ],
   "source": [
    "# loop over sticknets integers. 1 = 0101A, 2 = 0102A, ..., 24 = 0224A. \n",
    "# This lets you decide which sticknets you want to run the script for\n",
    "for i in [18,19,20,21,22,24]:\n",
    "    \n",
    "    probe_id = get_sticknet_ID(i)\n",
    "    print('-----\\n{}'.format(probe_id))\n",
    "    \n",
    "    # Grab StesoNet files from all all the reformatted directories (after the QC script)\n",
    "    dirs =  glob.glob(data_directory+probe_id+'*.txt')\n",
    "    \n",
    "    # sort files by date if you have more than one\n",
    "    if len(dirs) >1:\n",
    "        dirs = sorted(dirs,key=lambda f: dt.datetime.strptime(f[-19:-6],'%Y%m%d_%H%M'))\n",
    "    \n",
    "    # Initialize dataframe to hold all files from one probe\n",
    "    logs = pd.DataFrame()\n",
    "  \n",
    "    # loop over all files, aggregate them together in logs\n",
    "    for data_file in dirs:\n",
    "        \n",
    "        print('reading file {} ...'.format(data_file[-25:]))\n",
    "        \n",
    "        sndata = read_sn(data_file)\n",
    "        \n",
    "        print('     Data from {} to {}'.format(sndata.index[0].strftime('%d %b %Y %H%M UTC'),\n",
    "                                               sndata.index[-1].strftime('%d %b %Y %H%M UTC')))\n",
    "        \n",
    "        # if there are repeated times, drop them\n",
    "        sndata = sndata[~sndata.index.duplicated(keep='first')]\n",
    "        # replace missing values with np.nan\n",
    "        sndata= sndata.resample('100ms').asfreq() \n",
    "        \n",
    "        # group all files together\n",
    "        logs = pd.concat([logs, sndata])\n",
    "        \n",
    "    \n",
    "    # make sure data is in order\n",
    "    logs = logs.sort_index()\n",
    "\n",
    "    ##############################\n",
    "    # BIAS CORRECTIONS. COMMENT OUT IF YOU DONT WANT THIS\n",
    "#     tbias,rhbias,pbias = SN_biases_Meso1819(probe_id)\n",
    "\n",
    "#     logs['T'] = logs['T'] - tbias\n",
    "#     logs['RH'] = logs['RH'] - rhbias\n",
    "#     logs['P'] = logs['P'] - pbias\n",
    "    ##############################\n",
    "    \n",
    "    # important! makes sure days start at 0 utc (.date() ensures this). \n",
    "    start_date = pd.to_datetime(logs.index[0]).date()\n",
    "    end_date = pd.to_datetime(logs.index[-1]).date()+dt.timedelta(days=1)\n",
    "    \n",
    "    # create array of individual days\n",
    "    days = np.array([start_date+dt.timedelta(days=i) for i in range((end_date-start_date).days+1)])\n",
    "    \n",
    "    print('\\n start of all data:',start_date)\n",
    "    print(' end of all data:  ',end_date, '\\n')\n",
    "\n",
    "    # Parse each day and write to a .csv file witih format \"ProbeID_date.csv\"\n",
    "    for i in np.arange(0,len(days)-1,1):\n",
    "        \n",
    "        day_data = logs[days[i]:days[i+1]]\n",
    "        \n",
    "        day_data.to_csv('{0}{1}_{2}.csv'.format(output_directory,probe_id,days[i].strftime(\"%Y%m%d\")),\n",
    "                        float_format = '%.1f')\n",
    "        \n",
    "        print('Writing .csv file for {}'.format(days[i].strftime(\"%Y%m%d\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't want 10-Hz data? Average it over a different sampling frequency! (Not for EOL)\n",
    "\n",
    "The averaging function also adds max 3-s wind gust.\n",
    "\n",
    "If a flag is raised in ANY data point in the period you're averaging over, then that entire period will get flagged.\n",
    "\n",
    "To see the formats for the different sampling frequencies, look here:\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wind(ws,dir):\n",
    "    \"\"\" convert wind speed to u and v components (From Abby Hutson)\"\"\"\n",
    "    new_dir = 270-dir\n",
    "    u = (ws)*np.cos(new_dir * np.pi/180)\n",
    "    v = (ws)*np.sin(new_dir * np.pi/180)\n",
    "    return u,v\n",
    "\n",
    "def make_averaged_file(data, sampling_interval):\n",
    "    '''\n",
    "    data:                 pandas DataFrame of StickNet data\n",
    "    sampling_interval:    a time interval that can be accepted by pandas resample function. Examples\n",
    "                          include 1Min, 1s, 100ms, etc\n",
    "                          \n",
    "    This function averages data at whatever sampling interval was specified. A pandas DataFrame of the \n",
    "    averaged data is returned. \n",
    "    '''\n",
    "   \n",
    "    # variables\n",
    "    avg_temp = np.round(data['T'].resample(sampling_interval, label='right').mean(),1)\n",
    "    avg_P =    np.round(data['P'].resample(sampling_interval, label='right').mean(),1)\n",
    "    avg_RH =   np.round(data['RH'].resample(sampling_interval, label='right').mean(),1)\n",
    "    avg_ws =   np.round(data['windsp'].resample(sampling_interval, label='right').mean(),1)\n",
    "    times = avg_P.index\n",
    "    \n",
    "    # wind direction NOTE: can't do normal averaging for this, must break into u and v\n",
    "    u,v = convert_wind(data['windsp'], data['winddir'])\n",
    "    u_avg = u.resample(sampling_interval, label='right').mean()\n",
    "    v_avg = v.resample(sampling_interval, label='right').mean()\n",
    "    avg_wd = np.round(np.rad2deg(np.arctan2(u_avg, v_avg))+180,1) # rotate so 0 is N\n",
    "    \n",
    "    # max 3 second average wind gust\n",
    "    max_3sec_gust = np.round(data['windsp'].rolling('3s').mean().resample('1Min', label='right').max(),1)\n",
    "    \n",
    "    # if 10Hz data used in a minute has been flagged, the whole minute will be flagged\n",
    "    # if data looks wonky, checking this number may explain why\n",
    "    tflag = data['TFLAG'].resample(sampling_interval, label='right').max()\n",
    "    wflag = data['WFLAG'].resample(sampling_interval, label='right').max()\n",
    "    \n",
    "    # battery \n",
    "    batt = np.round(data['batt'].resample(sampling_interval, label='right').mean())\n",
    "\n",
    "    # put data into pandas dataframe\n",
    "    d = {'T':avg_temp, 'RH':avg_RH, 'P':avg_P, 'WS':avg_ws, 'WSMAX':max_3sec_gust, \n",
    "         'WD':avg_wd, 'BATT':batt, 'TFLAG':tflag, 'WFLAG':wflag}\n",
    "    avgmet = pd.DataFrame(data=d,index=times).replace(-999.9,np.nan)\n",
    "    avgmet.sort_index(ascending = True, inplace = True)\n",
    "    \n",
    "    return avgmet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From raw QCd data\n",
    "\n",
    "(If you have already made the EOL 10Hz data, then don't use this. Instead, just use the above function on the daily csv files you already made and write it back out to a csv with whatever name you want. It's much faster. I just havent taken the time to write that code out here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0219A\n",
      "reading file... data from 30 Nov 2018 2209 UTC to 10 Dec 2018 0835 UTC\n",
      "BIASES: Temperature -0.5, RH 3.0, Pressure 0.5\n",
      "start of all data: 2018-11-30\n",
      "end of all data: 2018-12-11\n",
      "Writing .csv file for 20181130\n",
      "Writing .csv file for 20181201\n",
      "Writing .csv file for 20181202\n",
      "Writing .csv file for 20181203\n",
      "Writing .csv file for 20181204\n",
      "Writing .csv file for 20181205\n",
      "Writing .csv file for 20181206\n",
      "Writing .csv file for 20181207\n",
      "Writing .csv file for 20181208\n",
      "Writing .csv file for 20181209\n",
      "Writing .csv file for 20181210\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# USER EDIT HERE =>\n",
    "\n",
    "SAMPLING_INTERVAL = '1Min'\n",
    "\n",
    "data_directory = r'/Users/jessmcd/Documents/StickNetQCtest/testdump/reformat/'\n",
    "output_directory= r'/Users/jessmcd/Documents/StickNetQCtest_dailys/'\n",
    "\n",
    "#####\n",
    "\n",
    "def SN_biases_Meso1819(steso):\n",
    "    #                    T .   RH .      P .   SP .  Dir\n",
    "    biases = {'0101A': [0.0,   0.0,    -0.5,    0.0,  0.0],\n",
    "              '0102A': [0.0,   -2.0,   0.0,    0.0,  0.0],\n",
    "              '0103A': [0.0,   0.75,   0.0,    0.0,  0.0],\n",
    "              '0104A': [0.0,   0.0,    -1.25,  0.0,  0.0],\n",
    "              '0105A': [-0.5,  0.5,    -1.0,   0.0,  0.0],\n",
    "              '0106A': [0.0,   2.5,    0.0,    0.0,  0.0],\n",
    "              '0107A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0108A': [0.0,   1.0,    0.0,    0.0,  0.0],\n",
    "              '0109A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0110A': [0.0,   0.0,    0.5,    0.0,  0.0],\n",
    "              '0111A': [0.0,   -1.25,  0.75,   0.0,  0.0],\n",
    "              '0112A': [0.0,   -0.75,  -0.5,   0.0,  0.0],\n",
    "              '0213A': [0.0,   -3.0,   -1.75,  0.0,  0.0],\n",
    "              '0214A': [0.0,   -0.5,   0.5,    0.0,  0.0],\n",
    "              '0215A': [0.0,   0.5,    -0.5,   0.0,  0.0],\n",
    "              '0216A': [0.0,   0.5,    0.5,    0.0,  0.0],\n",
    "              '0217A': [-0.5,  -1.0,   0.0,    0.0,  0.0],\n",
    "              '0218A': [0.0,   0.0,    0.5,    0.0,  0.0],\n",
    "              '0219A': [-0.5,  3.0,    0.5,    0.0,  0.0],\n",
    "              '0220A': [0.0,   0.0,    0.0,    0.0,  0.0],\n",
    "              '0221A': [0.5,   0.0,    1.75,   0.0,  0.0],\n",
    "              '0222A': [0.0,   4.75,   1.5,    0.0,  0.0],\n",
    "              '0223A': [0.0,   -0.75,  -1.0,   0.0,  0.0],\n",
    "              '0224A': [0.0,   0.0,    0.0,    0.0,  0.0]\n",
    "             }\n",
    "\n",
    "    tbias  = biases[steso[0:5]][0]\n",
    "    rhbias = biases[steso[0:5]][1]\n",
    "    pbias  = biases[steso[0:5]][2]\n",
    "\n",
    "    print('BIASES: Temperature {}, RH {}, Pressure {}'.format(tbias,rhbias,pbias))\n",
    "    \n",
    "    return tbias,rhbias,pbias\n",
    "\n",
    "#######################################################\n",
    "\n",
    "# loop over sticknets. This let you decide which sticknets you want to run the script for\n",
    "for i in [19]:\n",
    "    \n",
    "    probe_id = get_sticknet_ID(i)\n",
    "    print(probe_id)\n",
    "    \n",
    "    # Grab StesoNet files from all all the reformatted directories (after the QC script)\n",
    "    dirs = [os.path.abspath(i) for i in glob.glob(data_directory+probe_id+'*.txt')]\n",
    "    \n",
    "    # sort files by date if you have more than one\n",
    "    if len(dirs) >1:\n",
    "        dirs = sorted(dir,key=lambda f: dt.datetime.strptime(f[-19:-6],'%Y%m%d_%H%M'))\n",
    "    \n",
    "    # Initialize dataframe to hold all files from one probe\n",
    "    logs = pd.DataFrame()\n",
    "  \n",
    "    # loop over all files, aggregate them together in logs\n",
    "    for data_file in dirs:\n",
    "        \n",
    "        print('reading file...', end=' ')\n",
    "        \n",
    "        sndata = read_sn(data_file)\n",
    "        \n",
    "        print('data from {} to {}'.format(sndata.index[0].strftime('%d %b %Y %H%M UTC'),\n",
    "                                                          sndata.index[-1].strftime('%d %b %Y %H%M UTC')))\n",
    "        \n",
    "        sndata_averaged = make_averaged_file(sndata, SAMPLING_INTERVAL)\n",
    "        logs = pd.concat([logs, sndata_averaged])\n",
    "        \n",
    "\n",
    "    # make sure data is in order\n",
    "    logs = logs.sort_index()\n",
    "\n",
    "    ##############################\n",
    "    # BIAS CORRECTIONS. COMMENT OUT IF YOU DONT WANT THIS\n",
    "    tbias,rhbias,pbias = SN_biases_Meso1819(probe_id)\n",
    "\n",
    "    logs['T'] = logs['T'] - tbias\n",
    "    logs['RH'] = logs['RH'] - rhbias\n",
    "    logs['P'] = logs['P'] - pbias\n",
    "    ##############################\n",
    "    \n",
    "    # important! makes sure days start at 0 utc (.date() ensures this). \n",
    "    start_date = pd.to_datetime(logs.index[0]).date()\n",
    "    end_date = pd.to_datetime(logs.index[-1]).date()+dt.timedelta(days=1)\n",
    "    \n",
    "    # create array of individual days\n",
    "    days = np.array([start_date+dt.timedelta(days=i) for i in range((end_date-start_date).days+1)])\n",
    "    \n",
    "    print('start of all data:',start_date)\n",
    "    print('end of all data:',end_date)\n",
    "\n",
    "    # Parse each day and write to a .csv file witih format \"ProbeID_date.csv\"\n",
    "    for i in np.arange(0,len(days)-1,1):\n",
    "        \n",
    "        day_data = logs[days[i]:days[i+1]]\n",
    "        \n",
    "        day_data.to_csv('{0}{1}_{2}_{3}.csv'.format(output_directory,probe_id,days[i].strftime(\"%Y%m%d\"),\n",
    "                                                    SAMPLING_INTERVAL), float_format = '%.1f')\n",
    "        \n",
    "        print('Writing .csv file for {}'.format(days[i].strftime(\"%Y%m%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
